import time
import glob
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from transformers import BertTokenizerFast
from reformer_pytorch import ReformerLM, Autopadder
from mlm import MLM

from process_dataset import read_format, DocumentList
from nearest_neighbor import knn
from scatter_gather import scatter_gather_all


class DocumentDataset(Dataset):
    def __init__(self, path):
        """ Let N be number of documents and d embedding dimension:

            self.documents: DocumentList of N
            self.embeddings: N x d array
        """
        documents, embeddings = [], []
        for chunk in sorted(glob.glob(f'{path}/*')):
            # Only read name from one file
            if chunk.endswith('.npy'):
                continue
            name = chunk.rstrip('_doc.pl')
            _, chunk_documents, chunk_embeddings = read_format(name)
            documents.append(chunk_documents)
            embeddings.append(chunk_embeddings)

        self.documents = DocumentList(documents)
        self.embeddings = np.concatenate(embeddings)

    def __len__(self):
        return len(self.documents)

    def __getitem__(self, idx):
        return self.documents[idx], self.embeddings[idx]


latent_dataset = DocumentDataset('data/cc_news_smaller_processed_split/latent')


tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
tokenizer.max_len = 1024


class TaskDataset(Dataset):
    def __init__(self, path, latent=None):
        """ Let N be number of documents and k how many latent documents

            self.dataset: task dataset
            self.latent: type of latent document search
            self.references: N x k array of k document indicies
        """
        dataset = DocumentDataset(path)

        self.latent = latent
        if latent == 'nn':
            self.references = knn(dataset, latent_dataset)
        elif latent == 'scatter-gather':
            self.references = scatter_gather_all(dataset, latent_dataset)

        tokenized, self.stop_indicies = [], []
        for i, (document, _) in tqdm(enumerate(dataset)):
            text = document['text']
            # Usually important stuff in beginning, so we take at most first 512 of each
            text_tokenized = tokenizer.encode(text,
                                              max_length=tokenizer.max_len // 2,
                                              add_special_tokens=True,
                                              truncation=True)
            text_tokenized = torch.tensor(text_tokenized, dtype=torch.long)
            n_tokens = text_tokenized.shape[0]

            if self.latent:
                stop_indicies.append(n_tokens)
                # Take one document due to time limit
                reference = latent_dataset.documents[self.references[i][0]]
                reference_tokenized = tokenizer.encode(reference,
                                                       max_length=tokenizer.max_len // 2,
                                                       add_special_tokens=False,
                                                       truncation=True)
                reference_tokenized = torch.tensor(reference_tokenized, dtype=torch.long)
                text_tokenized = torch.cat((text_tokenized, reference_tokenzied))

            num_pad = tokenizer.max_len - n_tokens
            text_tokenized = torch.cat((text_tokenized, torch.zeros(num_pad, dtype=torch.long)))
            tokenized.append(text_tokenized)

        self.tokenized = torch.stack(tokenized)
        print('Train Dataset Size: ', self.tokenized.shape)

    def __len__(self):
        return len(self.tokenized)

    def __getitem__(self, idx):
        if self.latent:
            return self.tokenized[idx], self.stop_indicies[idx]
        return self.tokenized[idx]


def load_batch(batch, latent=False):
    """ Processs batch generated by dataloader of task dataset for model usage """
    if latent:
        tokenized, stop_index = batch
    else:
        tokenized, stop_index = batch, None
    if torch.cuda.is_available():
        tokenized = tokenized.cuda()
        if latent:
            stop_index = stop_index.cuda()
    return tokenized, stop_index


def run_experiment(name, latent=None, epochs=200, plot=True):
    """ Run train experiment given latent database structure

        plot visualizes the train and val losses
    """
    task_dataset = TaskDataset(
        'data/cc_news_smaller_processed_split/train', latent=latent)
    train_dataset, validation_dataset = \
        torch.utils.data.random_split(
            task_dataset, [9000, len(task_dataset) - 9000])

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=True)

    # if latent, no reference document so can halve max_seq_len
    seq_len = tokenizer.max_len // 2 if latent else tokenizer.max_len

    transformer = ReformerLM(
        num_tokens = tokenizer.vocab_size,
        dim = 512,
        depth = 6,
        max_seq_len = seq_len,
        causal = True,
        use_full_attn = False,
    )
    transformer = Autopadder(transformer)

    trainer = MLM(transformer,
                  mask_token_id=103,
                  pad_token_id=0,
                  mask_prob=0.15,
                  replace_prob=0.90,
                  mask_ignore_token_ids = [101, 102])

    if torch.cuda.is_available():
        trainer = trainer.cuda()

    optimizer = torch.optim.Adam(trainer.parameters(), lr=3e-4)

    train_losses, validation_losses = [], []

    for epoch in range(epochs):
        print(f'Epoch-{epoch}')

        train_loss, validation_loss = 0, 0

        transformer.train()
        for i, batch in enumerate(tqdm(train_loader)):
            tokenized, stop_index = load_batch(batch, latent)
            optimizer.zero_grad()
            loss = trainer(tokenized, stop_index)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        transformer.eval()
        for i, batch in enumerate(tqdm(validation_loader)):
            tokenized, stop_index = load_batch(batch, latent)
            loss = trainer(tokenized, stop_index)
            validation_loss += loss.item()

        print(f'Train Loss: {train_loss}')
        print(f'Validation Loss: {validation_loss}')

        train_losses.append(train_loss)
        validation_losses.append(validation_loss)

        if epoch % 5 == 0:
            torch.save(transformer, f'checkpoints/{name}_{epoch:03d}_model.pt')

    print(train_losses)
    print(validation_losses)

    if plot:
        plt.plot(train_losses)
        plt.plot(val_losses)
        plt.savefig(f'{name}_losses.png')


def main():
    run_experiment('no_latent')
    #run_experiment('scann')
    #run_experiment('scatter_gather')


if __name__ == '__main__':
    main()
